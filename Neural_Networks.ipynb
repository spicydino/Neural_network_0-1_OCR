{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1bUvDZBnisj6","mYi99AacivWS","mncx2Xuyi0_g","Jce08PiYi-a5","Xtj1H1_MlVrq","eys3sg-rlh_5","la7GOauPln_w"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Neural Networks 0-1"],"metadata":{"id":"1bUvDZBnisj6"}},{"cell_type":"markdown","source":["\n","## Lets start with computing output of a single neuron. the formula goes as:\n","\n","$$ \\text{neuron output} = \\mathbf{input} \\cdot \\mathbf{weight} + bias $$\n"],"metadata":{"id":"mYi99AacivWS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zv3O0qXQhft9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722941843700,"user_tz":-330,"elapsed":4,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}},"outputId":"9ad54043-7f87-4cd5-d5b7-b876c1fe0324"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0107588494"]},"metadata":{},"execution_count":1}],"source":["X = 0.67378\n","w = 0.54623\n","b = 0.64272\n","nop = w * X + b\n","nop"]},{"cell_type":"markdown","source":["## Now, lets compute multiple neurons at once, also known as a layer of neurons.\n","\n","$$ \\text{output} = \\sum_{i=0}^{n} (\\text{input}_i \\cdot \\text{weight}_i) + \\text{biases} $$\n","\n","Here input and weight are arrays of float values of which dot product is calculated."],"metadata":{"id":"mncx2Xuyi0_g"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Kyon6P1Wi2yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = [0.34 , 0.45 , 0.69]\n","w = [0.24 , 0.53 , 0.78]\n","b = 0.72\n","op = np.dot(X,w)+b\n","print(op)\n"],"metadata":{"id":"gyP8tJMHi53R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722943334700,"user_tz":-330,"elapsed":418,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}},"outputId":"8a8b8b5e-40e4-4fdc-e09c-127ddbc0cd81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.5783\n"]}]},{"cell_type":"markdown","source":["## ReLU Activation Function.\n","$$ \\text{ReLU}(x) = \\max(0, x) $$\n","\n","ReLU is mostly used to activate hidden layers."],"metadata":{"id":"Jce08PiYi-a5"}},{"cell_type":"code","source":["ReLu = np.maximum(0, op)\n","ReLu"],"metadata":{"id":"95wJBiu1i-9F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722944037088,"user_tz":-330,"elapsed":395,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}},"outputId":"79346e44-6456-42ee-d733-62428ef1798e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.5783"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## SoftMax Activation\n","\n","$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $$"],"metadata":{"id":"Xtj1H1_MlVrq"}},{"cell_type":"code","source":["inputs = np.random.randn(1,4)\n","exp_value = np.exp(inputs - np.max(inputs,axis=1,keepdims = True))\n","probabilities = exp_value / np.sum(exp_value,axis=1,keepdims= True)\n","probabilities"],"metadata":{"id":"zsRBJ5B7lcTg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722945844875,"user_tz":-330,"elapsed":394,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}},"outputId":"6057a6c5-d27b-4f9a-d0f0-f1c11eb106d3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.13949236, 0.49890119, 0.28791965, 0.07368679]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## Loss Calculation (Categorical Cross-Entropy)\n","\n","$$ \\text{Loss} = -\\sum_{i} y_i \\log(p_i) $$"],"metadata":{"id":"eys3sg-rlh_5"}},{"cell_type":"code","source":["true_value = np.array([0,1,0,0])\n","probabilities = np.clip(probabilities, 1e-15, 1 -1e-15)\n","loss = -np.sum(true_value * np.log(probabilities))\n","loss"],"metadata":{"id":"e-iUXao6mVJ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722946812916,"user_tz":-330,"elapsed":435,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}},"outputId":"8e5a49ed-c7e9-4ec9-84a3-8d3e919870d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6953472154376993"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Adam Optimizer\n","\n","### Compute the biased first moment estimate $m_t$\n","$$\n","m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n","$$\n","\n","- where $g_t$ is the gradient at time step $t$.\n","\n","### Compute the biased second raw moment estimate $v_t$:\n","\n","$$\n","v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n","$$\n","### Compute bias-corrected first moment estimate $\\hat{m}^t$:\n","\n","$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n","\n","### Compute bias-corrected second raw moment estimate $\\hat{v}^t$:\n","$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n","\n","### Update the parameters $Î¸$:\n","$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha \\cdot \\hat{v}_t}{\\sqrt{\\hat{m}_t} + \\epsilon}$$"],"metadata":{"id":"la7GOauPln_w"}},{"cell_type":"code","source":[],"metadata":{"id":"Ii8Qjmw3lnKv","executionInfo":{"status":"ok","timestamp":1723028647931,"user_tz":-330,"elapsed":5,"user":{"displayName":"ADITYA KOPARKAR","userId":"09295803290926774394"}}},"execution_count":null,"outputs":[]}]}